{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6d31c50-71f1-46d3-bab8-9f00a59ee8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Applications/miniconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Applications/miniconda3/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Applications/miniconda3/lib/python3.12/site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Applications/miniconda3/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Applications/miniconda3/lib/python3.12/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a91bd65b-1a73-4f23-828a-8b14c3194495",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Hello welcome, to odera nnaji's NLP Tutorial. \n",
    "Please do watch the eniter course! to become an expert in NLP.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5c56516-0177-4bd6-8c4b-53f46f53ba87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome, to odera nnaji's NLP Tutorial. \n",
      "Please do watch the eniter course! to become an expert in NLP.\n"
     ]
    }
   ],
   "source": [
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72942c9d-202a-487c-ac00-bf8dc3002640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello welcome, to odera nnaji's NLP Tutorial.\",\n",
       " 'Please do watch the eniter course!',\n",
       " 'to become an expert in NLP.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da3fa6-eeb0-4980-81fa-3a5be4fd6fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a3628ec-c5fd-449a-8e15-226eabfd3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write out a Corpus(Paragraph)\n",
    "corpus = \"\"\"The sun dipped below the horizon, painting the sky in hues of orange and pink. \n",
    "Birds chirped their evening songs! signaling the end of another day. \n",
    "A gentle breeze rustled through the trees, carrying the scent of blooming flowers. \n",
    "The world seemed to pause, embracing the tranquility of dusk.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab5327b5-e4c3-4f72-8ea5-da08796846b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun dipped below the horizon, painting the sky in hues of orange and pink. \n",
      "Birds chirped their evening songs! signaling the end of another day. \n",
      "A gentle breeze rustled through the trees, carrying the scent of blooming flowers. \n",
      "The world seemed to pause, embracing the tranquility of dusk.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "472e9cd9-8a5c-45d5-bdd9-acce60cc1ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tokenization\n",
    "##Paragraph(Corpus) --> sentence\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da02ba75-7680-4213-b951-7d54571afce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The sun dipped below the horizon, painting the sky in hues of orange and pink.',\n",
       " 'Birds chirped their evening songs!',\n",
       " 'signaling the end of another day.',\n",
       " 'A gentle breeze rustled through the trees, carrying the scent of blooming flowers.',\n",
       " 'The world seemed to pause, embracing the tranquility of dusk.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2fd586f6-2bee-4ae8-9f04-789e3a372b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = sent_tokenize(corpus)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35a2d64b-1984-415f-981a-a8df06763da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun dipped below the horizon, painting the sky in hues of orange and pink.\n",
      "Birds chirped their evening songs!\n",
      "signaling the end of another day.\n",
      "A gentle breeze rustled through the trees, carrying the scent of blooming flowers.\n",
      "The world seemed to pause, embracing the tranquility of dusk.\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0c98e-0355-4a5b-840b-8ca97bc37c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83231a2a-5784-4dda-902e-3928b3e89c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Tokenization\n",
    "### Corpus/paragraph  --> words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2520939-8bce-43ba-a0f9-a87cf0535ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97221a47-a95e-4f0a-ace5-5eabfa40eb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sun', 'dipped', 'below', 'the', 'horizon', ',', 'painting', 'the', 'sky', 'in', 'hues', 'of', 'orange', 'and', 'pink', '.', 'Birds', 'chirped', 'their', 'evening', 'songs', '!', 'signaling', 'the', 'end', 'of', 'another', 'day', '.', 'A', 'gentle', 'breeze', 'rustled', 'through', 'the', 'trees', ',', 'carrying', 'the', 'scent', 'of', 'blooming', 'flowers', '.', 'The', 'world', 'seemed', 'to', 'pause', ',', 'embracing', 'the', 'tranquility', 'of', 'dusk', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(corpus)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53706a5f-8206-4486-b25a-b469e2866620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7287c29-df57-4b1e-bba7-e532fc60b20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sun', 'dipped', 'below', 'the', 'horizon', ',', 'painting', 'the', 'sky', 'in', 'hues', 'of', 'orange', 'and', 'pink', '.']\n",
      "['Birds', 'chirped', 'their', 'evening', 'songs', '!']\n",
      "['signaling', 'the', 'end', 'of', 'another', 'day', '.']\n",
      "['A', 'gentle', 'breeze', 'rustled', 'through', 'the', 'trees', ',', 'carrying', 'the', 'scent', 'of', 'blooming', 'flowers', '.']\n",
      "['The', 'world', 'seemed', 'to', 'pause', ',', 'embracing', 'the', 'tranquility', 'of', 'dusk', '.']\n"
     ]
    }
   ],
   "source": [
    "### when you word tokenise a sentence\n",
    "\n",
    "for sentence in doc:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d81dc7b-08e6-4aa4-a12f-32484c7e8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "###wordpunct --Removes punctuations\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77e60207-44d6-4ae7-8553-46464286942f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sun', 'dipped', 'below', 'the', 'horizon', ',', 'painting', 'the', 'sky', 'in', 'hues', 'of', 'orange', 'and', 'pink', '.', 'Birds', 'chirped', 'their', 'evening', 'songs', '!', 'signaling', 'the', 'end', 'of', 'another', 'day', '.', 'A', 'gentle', 'breeze', 'rustled', 'through', 'the', 'trees', ',', 'carrying', 'the', 'scent', 'of', 'blooming', 'flowers', '.', 'The', 'world', 'seemed', 'to', 'pause', ',', 'embracing', 'the', 'tranquility', 'of', 'dusk', '.']\n"
     ]
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)\n",
    "\n",
    "doc1 = wordpunct_tokenize(corpus)\n",
    "print(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de8cd3-370f-40e3-bf6e-8d6f36ed1903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "779527ef-2011-4489-9062-e6b820cec6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### - TreebankWordTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58268b05-235f-4fe6-9d6d-fd3386ee3714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d2a8e94-20d2-4af8-aac3-7878d506ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d5ec6e6-3f1d-4610-be5e-62281e160915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sun', 'dipped', 'below', 'the', 'horizon', ',', 'painting', 'the', 'sky', 'in', 'hues', 'of', 'orange', 'and', 'pink.', 'Birds', 'chirped', 'their', 'evening', 'songs', '!', 'signaling', 'the', 'end', 'of', 'another', 'day.', 'A', 'gentle', 'breeze', 'rustled', 'through', 'the', 'trees', ',', 'carrying', 'the', 'scent', 'of', 'blooming', 'flowers.', 'The', 'world', 'seemed', 'to', 'pause', ',', 'embracing', 'the', 'tranquility', 'of', 'dusk', '.']\n"
     ]
    }
   ],
   "source": [
    "x = tokenize.tokenize(corpus)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "895ddc90-be9a-4101-b8fe-609cc6f0065e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'h', 'e', ' ', 's', 'u', 'n', ' ', 'd', 'i', 'p', 'p', 'e', 'd', ' ', 'b', 'e', 'l', 'o', 'w', ' ', 't', 'h', 'e', ' ', 'h', 'o', 'r', 'i', 'z', 'o', 'n', ',', ' ', 'p', 'a', 'i', 'n', 't', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 's', 'k', 'y', ' ', 'i', 'n', ' ', 'h', 'u', 'e', 's', ' ', 'o', 'f', ' ', 'o', 'r', 'a', 'n', 'g', 'e', ' ', 'a', 'n', 'd', ' ', 'p', 'i', 'n', 'k', '.', ' ', '\\n', 'B', 'i', 'r', 'd', 's', ' ', 'c', 'h', 'i', 'r', 'p', 'e', 'd', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'e', 'v', 'e', 'n', 'i', 'n', 'g', ' ', 's', 'o', 'n', 'g', 's', '!', ' ', 's', 'i', 'g', 'n', 'a', 'l', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'e', 'n', 'd', ' ', 'o', 'f', ' ', 'a', 'n', 'o', 't', 'h', 'e', 'r', ' ', 'd', 'a', 'y', '.', ' ', '\\n', 'A', ' ', 'g', 'e', 'n', 't', 'l', 'e', ' ', 'b', 'r', 'e', 'e', 'z', 'e', ' ', 'r', 'u', 's', 't', 'l', 'e', 'd', ' ', 't', 'h', 'r', 'o', 'u', 'g', 'h', ' ', 't', 'h', 'e', ' ', 't', 'r', 'e', 'e', 's', ',', ' ', 'c', 'a', 'r', 'r', 'y', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 's', 'c', 'e', 'n', 't', ' ', 'o', 'f', ' ', 'b', 'l', 'o', 'o', 'm', 'i', 'n', 'g', ' ', 'f', 'l', 'o', 'w', 'e', 'r', 's', '.', ' ', '\\n', 'T', 'h', 'e', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 's', 'e', 'e', 'm', 'e', 'd', ' ', 't', 'o', ' ', 'p', 'a', 'u', 's', 'e', ',', ' ', 'e', 'm', 'b', 'r', 'a', 'c', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 't', 'r', 'a', 'n', 'q', 'u', 'i', 'l', 'i', 't', 'y', ' ', 'o', 'f', ' ', 'd', 'u', 's', 'k', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(pattern=\".\")  # \".\" matches any character\n",
    "char_tokens = tokenizer.tokenize(corpus)\n",
    "\n",
    "print(char_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6187c8-faeb-4e70-9f78-2a72d3049064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
