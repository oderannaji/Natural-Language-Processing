{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4999ee5d-539f-4b5d-8d65-af632fe5e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab106732-186c-4339-869c-58395f22a7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'what', b'you', b'know', b'you', b\"can't\", b'explain,', b'but', b'you', b'feel', b'it.']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize([\"what you know you can't explain, but you feel it.\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99cac358-3f8b-4587-9fd1-ae3338919e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'what', b'you', b'know', b'you', b'can', b\"'\", b't', b'explain', b',', b'but', b'you', b'feel', b'it', b'.']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
    "token = tokenizer.tokenize([\"what you know you can't explain, but you feel it.\"])\n",
    "print(token.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9b6ab1-a8c4-4f14-a694-9fe00a206ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'what', b'you', b'know', b'you', b\"can't\", b'explain,', b'but', b'you', b'feel', b'it.']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize([\"what you know you can't explain, but you feel it.\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01fd1ff9-6531-4b06-a8c9-2786adf62108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52382"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_wp_en_vocab.txt?raw=true\"\n",
    "r = requests.get(url)\n",
    "filepath = \"vocab.txt\"\n",
    "open(filepath, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74b478d0-cca9-45f2-9382-c7b6f213074b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[b'what'], [b'you'], [b'know'], [b'you'], [b\"can't\"], [b'explain,'], [b'but'], [b'you'], [b'feel'], [b'it.']]]\n"
     ]
    }
   ],
   "source": [
    "subtokenizer = tf_text.WordpieceTokenizer(filepath)\n",
    "subtokens = tokenizer.tokenize(tokens)\n",
    "print(subtokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f58bb673-ed99-41e7-8926-81adde6e27cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[b'what'], [b'you'], [b'know'], [b'you'], [b'can'], [b\"'\"], [b't'], [b'explain'], [b','], [b'but'], [b'you'], [b'feel'], [b'it'], [b'.']]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.BertTokenizer(filepath, token_out_type=tf.string, lower_case=True)\n",
    "tokens = tokenizer.tokenize([\"what you know you can't explain, but you feel it.\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54015cbb-fdc6-43b7-9e6d-baafa011641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_oss_model.model?raw=true\"\n",
    "sp_model = requests.get(url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b1be17a-2bf7-4db6-ac2c-0d15b3f10149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'\\xe2\\x96\\x81what', b'\\xe2\\x96\\x81you', b'\\xe2\\x96\\x81know', b'\\xe2\\x96\\x81you', b'\\xe2\\x96\\x81can', b\"'\", b't', b'\\xe2\\x96\\x81explain', b',', b'\\xe2\\x96\\x81but', b'\\xe2\\x96\\x81you', b'\\xe2\\x96\\x81feel', b'\\xe2\\x96\\x81it', b'.']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.SentencepieceTokenizer(sp_model, out_type=tf.string)\n",
    "tokens = tokenizer.tokenize([\"what you know you can't explain, but you feel it.\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdaa82a5-e5fb-4431-aff3-84a01838c184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119, 104, 97, 116, 32, 121, 111, 117, 32, 107, 110, 111, 119, 32, 121, 111, 117, 32, 99, 97, 110, 39, 116, 32, 101, 120, 112, 108, 97, 105, 110, 44, 32, 98, 117, 116, 32, 121, 111, 117, 32, 102, 101, 101, 108, 32, 105, 116, 46]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.UnicodeCharTokenizer()\n",
    "tokens = tokenizer.tokenize([\"what you know you can't explain, but you feel it.\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45034321-a016-4aeb-9e35-61dcb437eed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'wh', b'ha', b'at', b't ', b' y', b'yo', b'ou', b'u ', b' k', b'kn', b'no', b'ow', b'w ', b' y', b'yo', b'ou', b'u ', b' c', b'ca', b'an', b\"n'\", b\"'t\", b't ', b' e', b'ex', b'xp', b'pl', b'la', b'ai', b'in', b'n,', b', ', b' b', b'bu', b'ut', b't ', b' y', b'yo', b'ou', b'u ', b' f', b'fe', b'ee', b'el', b'l ', b' i', b'it', b't.']]\n"
     ]
    }
   ],
   "source": [
    "characters = tf.strings.unicode_encode(tf.expand_dims(tokens, -1), \"UTF-8\")\n",
    "bigrams = tf_text.ngrams(characters, 2, reduction_type=tf_text.Reduction.STRING_JOIN, string_separator='')\n",
    "print(bigrams.to_list())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44f2f772-f3ce-48e0-8459-2b5f9c6e850b",
   "metadata": {},
   "source": [
    "strings = [\"新华社北京\"]\n",
    "labels = [[0, 1, 1, 0, 1]]\n",
    "tokenizer = tf_text.SplitMergeTokenizer()\n",
    "tokens = tokenizer.tokenize(strings, labels)\n",
    "print(decode_utf8_tensor(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d83eb-6985-4e4a-8a2a-274285416562",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_HANDLE = \"https://tfhub.dev/google/zh_segmentation/1\"\n",
    "segmenter = tf_text.HubModuleTokenizer(MODEL_HANDLE)\n",
    "tokens = segmenter.tokenize([\"新华社北京\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b076f164-8795-4f6d-a5bb-65399115063a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[None, None, None, None, None, None, None, None, None, None]]\n"
     ]
    }
   ],
   "source": [
    "def decode_list(x):\n",
    "  if type(x) is list:\n",
    "    return list(map(decode_list, x))\n",
    "    return x.decode(\"UTF-8\")\n",
    "\n",
    "def decode_utf8_tensor(x):\n",
    "  return list(map(decode_list, x.to_list()))\n",
    "\n",
    "print(decode_utf8_tensor(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0176852-5af6-4546-8789-b8ca1893cc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[None, None]]\n"
     ]
    }
   ],
   "source": [
    "strings = [\"新华社北京\"]\n",
    "labels = [[0, 1, 1, 0, 1]]\n",
    "tokenizer = tf_text.SplitMergeTokenizer()\n",
    "tokens = tokenizer.tokenize(strings, labels)\n",
    "print(decode_utf8_tensor(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "186ee2c8-f2b5-4e7f-bba3-6455d0f2a510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[None, None]]\n"
     ]
    }
   ],
   "source": [
    "strings = [[\"新华社北京\"]]\n",
    "labels = [[[5.0, -3.2], [0.2, 12.0], [0.0, 11.0], [2.2, -1.0], [-3.0, 3.0]]]\n",
    "tokenizer = tf_text.SplitMergeFromLogitsTokenizer()\n",
    "tokenizer.tokenize(strings, labels)\n",
    "print(decode_utf8_tensor(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a3a6cf5-aab4-4573-acf7-e020a4be4343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'What', b'you', b'know', b'you', b\"can't\", b'explain,', b'but', b'you', b'feel', b'it.']]\n"
     ]
    }
   ],
   "source": [
    "splitter = tf_text.RegexSplitter(r\"\\s\")\n",
    "tokens = splitter.split([\"What you know you can't explain, but you feel it.\"], )\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "979b4226-0dad-49cb-8cfa-fbd4cabc1feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Everything', b'not', b'saved', b'will', b'be', b'lost', b'.']]\n",
      "[[0, 11, 15, 21, 26, 29, 33]]\n",
      "[[10, 14, 20, 25, 28, 33, 34]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
    "(tokens, start_offsets, end_offsets) = tokenizer.tokenize_with_offsets(['Everything not saved will be lost.'])\n",
    "print(tokens.to_list())\n",
    "print(start_offsets.to_list())\n",
    "print(end_offsets.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cea6ed93-ab4b-40ca-8d66-3b3b11ca02c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[87, 104, 97, 116, 32, 121, 111, 117, 32, 107, 110, 111, 119, 32, 121, 111, 117, 32, 99, 97, 110, 39, 116, 32, 101, 120, 112, 108, 97, 105, 110, 44, 32, 98, 117, 116, 32, 121, 111, 117, 32, 102, 101, 101, 108, 32, 105, 116, 46]]\n",
      "[b\"What you know you can't explain, but you feel it.\"]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.UnicodeCharTokenizer()\n",
    "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
    "print(tokens.to_list())\n",
    "strings = tokenizer.detokenize(tokens)\n",
    "print(strings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a00898ee-2aeb-4381-85c8-69c90b10a749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Never', b'tell', b'me', b'the', b'odds.']]\n",
      "[[b\"It's\", b'a', b'trap!']]\n"
     ]
    }
   ],
   "source": [
    "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
    "tokenizer = tf_text.WhitespaceTokenizer()\n",
    "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
    "iterator = iter(tokenized_docs)\n",
    "print(next(iterator).to_list())\n",
    "print(next(iterator).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcec7d7-aec4-4d34-9aba-367556a3efed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661689a-9b89-44e7-841d-deeca875ded1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebddcc3-fda7-455a-80f6-9d750795b078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892befa9-d815-4c24-b1b5-6c422aea61b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e52e707-37ee-47ba-b643-be03f0c4c100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d65248-36eb-4552-8c82-435b559b9b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbcd79-c028-49f3-81ff-6450447cda36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8005bf-32e3-485c-9086-f958a1c40ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38543f71-55b2-4e47-9bcc-21156edded74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68f5ed-6fb4-4324-a79a-a01ebf895d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd18963-f593-42b2-b98b-a75053d0747a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07844a5-24db-4b51-9eba-0ee66d835c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b62ea0-36dd-48cb-92a1-9a1975e780c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b03e9-7cdb-4abc-846e-379a57f68518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
